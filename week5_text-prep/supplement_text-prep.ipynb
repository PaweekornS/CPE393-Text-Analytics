{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvkDgZZn0tgu"
      },
      "source": [
        "# Supplementary: Text Data Preparation\n",
        "\n",
        "Objectives:\n",
        "- To introduce students to different text pre-processing techniques.\n",
        "- Students will gain hands-on experience through examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g528JDAv0tgw"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5nswfU4y0tgw"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoqGRm3Y0tgx"
      },
      "source": [
        "## 1) Text Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9op1qtCL0tgx"
      },
      "source": [
        "### 1.1) Noise Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veVNqJKQ0tgx"
      },
      "source": [
        "#### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "63BK6fku0tgy"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO-7TWfL0tgy"
      },
      "source": [
        "Here are examples of stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4k6VTB-N0tgy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Iij9nfCc0tgy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
          ]
        }
      ],
      "source": [
        "print(stopwords.words('french'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "deK8bwGZ0tgy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'dass', 'daß', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen']\n"
          ]
        }
      ],
      "source": [
        "print(stopwords.words('german'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "olaH4vHw0tgy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['一', '一下', '一些', '一切', '一则', '一天', '一定', '一方面', '一旦', '一时', '一来', '一样', '一次', '一片', '一直', '一致', '一般', '一起', '一边', '一面', '万一', '上下', '上升', '上去', '上来', '上述', '上面', '下列', '下去', '下来', '下面', '不一', '不久', '不仅', '不会', '不但', '不光', '不单', '不变', '不只', '不可', '不同', '不够', '不如', '不得', '不怕', '不惟', '不成', '不拘', '不敢', '不断', '不是', '不比', '不然', '不特', '不独', '不管', '不能', '不要', '不论', '不足', '不过', '不问', '与', '与其', '与否', '与此同时', '专门', '且', '两者', '严格', '严重', '个', '个人', '个别', '中小', '中间', '丰富', '临', '为', '为主', '为了', '为什么', '为什麽', '为何', '为着', '主张', '主要', '举行', '乃', '乃至', '么', '之', '之一', '之前', '之后', '之後', '之所以', '之类', '乌乎', '乎', '乘', '也', '也好', '也是', '也罢', '了', '了解', '争取', '于', '于是', '于是乎', '云云', '互相', '产生', '人们', '人家', '什么', '什么样', '什麽', '今后', '今天', '今年', '今後', '仍然', '从', '从事', '从而', '他', '他人', '他们', '他的', '代替', '以', '以上', '以下', '以为', '以便', '以免', '以前', '以及', '以后', '以外', '以後', '以来', '以至', '以至于', '以致', '们', '任', '任何', '任凭', '任务', '企图', '伟大', '似乎', '似的', '但', '但是', '何', '何况', '何处', '何时', '作为', '你', '你们', '你的', '使得', '使用', '例如', '依', '依照', '依靠', '促进', '保持', '俺', '俺们', '倘', '倘使', '倘或', '倘然', '倘若', '假使', '假如', '假若', '做到', '像', '允许', '充分', '先后', '先後', '先生', '全部', '全面', '兮', '共同', '关于', '其', '其一', '其中', '其二', '其他', '其余', '其它', '其实', '其次', '具体', '具体地说', '具体说来', '具有', '再者', '再说', '冒', '冲', '决定', '况且', '准备', '几', '几乎', '几时', '凭', '凭借', '出去', '出来', '出现', '分别', '则', '别', '别的', '别说', '到', '前后', '前者', '前进', '前面', '加之', '加以', '加入', '加强', '十分', '即', '即令', '即使', '即便', '即或', '即若', '却不', '原来', '又', '及', '及其', '及时', '及至', '双方', '反之', '反应', '反映', '反过来', '反过来说', '取得', '受到', '变成', '另', '另一方面', '另外', '只是', '只有', '只要', '只限', '叫', '叫做', '召开', '叮咚', '可', '可以', '可是', '可能', '可见', '各', '各个', '各人', '各位', '各地', '各种', '各级', '各自', '合理', '同', '同一', '同时', '同样', '后来', '后面', '向', '向着', '吓', '吗', '否则', '吧', '吧哒', '吱', '呀', '呃', '呕', '呗', '呜', '呜呼', '呢', '周围', '呵', '呸', '呼哧', '咋', '和', '咚', '咦', '咱', '咱们', '咳', '哇', '哈', '哈哈', '哉', '哎', '哎呀', '哎哟', '哗', '哟', '哦', '哩', '哪', '哪个', '哪些', '哪儿', '哪天', '哪年', '哪怕', '哪样', '哪边', '哪里', '哼', '哼唷', '唉', '啊', '啐', '啥', '啦', '啪达', '喂', '喏', '喔唷', '嗡嗡', '嗬', '嗯', '嗳', '嘎', '嘎登', '嘘', '嘛', '嘻', '嘿', '因', '因为', '因此', '因而', '固然', '在', '在下', '地', '坚决', '坚持', '基本', '处理', '复杂', '多', '多少', '多数', '多次', '大力', '大多数', '大大', '大家', '大批', '大约', '大量', '失去', '她', '她们', '她的', '好的', '好象', '如', '如上所述', '如下', '如何', '如其', '如果', '如此', '如若', '存在', '宁', '宁可', '宁愿', '宁肯', '它', '它们', '它们的', '它的', '安全', '完全', '完成', '实现', '实际', '宣布', '容易', '密切', '对', '对于', '对应', '将', '少数', '尔后', '尚且', '尤其', '就', '就是', '就是说', '尽', '尽管', '属于', '岂但', '左右', '巨大', '巩固', '己', '已经', '帮助', '常常', '并', '并不', '并不是', '并且', '并没有', '广大', '广泛', '应当', '应用', '应该', '开外', '开始', '开展', '引起', '强烈', '强调', '归', '当', '当前', '当时', '当然', '当着', '形成', '彻底', '彼', '彼此', '往', '往往', '待', '後来', '後面', '得', '得出', '得到', '心里', '必然', '必要', '必须', '怎', '怎么', '怎么办', '怎么样', '怎样', '怎麽', '总之', '总是', '总的来看', '总的来说', '总的说来', '总结', '总而言之', '恰恰相反', '您', '意思', '愿意', '慢说', '成为', '我', '我们', '我的', '或', '或是', '或者', '战斗', '所', '所以', '所有', '所谓', '打', '扩大', '把', '抑或', '拿', '按', '按照', '换句话说', '换言之', '据', '掌握', '接着', '接著', '故', '故此', '整个', '方便', '方面', '旁人', '无宁', '无法', '无论', '既', '既是', '既然', '时候', '明显', '明确', '是', '是否', '是的', '显然', '显著', '普通', '普遍', '更加', '曾经', '替', '最后', '最大', '最好', '最後', '最近', '最高', '有', '有些', '有关', '有利', '有力', '有所', '有效', '有时', '有点', '有的', '有着', '有著', '望', '朝', '朝着', '本', '本着', '来', '来着', '极了', '构成', '果然', '果真', '某', '某个', '某些', '根据', '根本', '欢迎', '正在', '正如', '正常', '此', '此外', '此时', '此间', '毋宁', '每', '每个', '每天', '每年', '每当', '比', '比如', '比方', '比较', '毫不', '没有', '沿', '沿着', '注意', '深入', '清楚', '满足', '漫说', '焉', '然则', '然后', '然後', '然而', '照', '照着', '特别是', '特殊', '特点', '现代', '现在', '甚么', '甚而', '甚至', '用', '由', '由于', '由此可见', '的', '的话', '目前', '直到', '直接', '相似', '相信', '相反', '相同', '相对', '相对而言', '相应', '相当', '相等', '省得', '看出', '看到', '看来', '看看', '看见', '真是', '真正', '着', '着呢', '矣', '知道', '确定', '离', '积极', '移动', '突出', '突然', '立即', '第', '等', '等等', '管', '紧接着', '纵', '纵令', '纵使', '纵然', '练习', '组成', '经', '经常', '经过', '结合', '结果', '给', '绝对', '继续', '继而', '维持', '综上所述', '罢了', '考虑', '者', '而', '而且', '而况', '而外', '而已', '而是', '而言', '联系', '能', '能否', '能够', '腾', '自', '自个儿', '自从', '自各儿', '自家', '自己', '自身', '至', '至于', '良好', '若', '若是', '若非', '范围', '莫若', '获得', '虽', '虽则', '虽然', '虽说', '行为', '行动', '表明', '表示', '被', '要', '要不', '要不是', '要不然', '要么', '要是', '要求', '规定', '觉得', '认为', '认真', '认识', '让', '许多', '论', '设使', '设若', '该', '说明', '诸位', '谁', '谁知', '赶', '起', '起来', '起见', '趁', '趁着', '越是', '跟', '转动', '转变', '转贴', '较', '较之', '边', '达到', '迅速', '过', '过去', '过来', '运用', '还是', '还有', '这', '这个', '这么', '这么些', '这么样', '这么点儿', '这些', '这会儿', '这儿', '这就是说', '这时', '这样', '这点', '这种', '这边', '这里', '这麽', '进入', '进步', '进而', '进行', '连', '连同', '适应', '适当', '适用', '逐步', '逐渐', '通常', '通过', '造成', '遇到', '遭到', '避免', '那', '那个', '那么', '那么些', '那么样', '那些', '那会儿', '那儿', '那时', '那样', '那边', '那里', '那麽', '部分', '鄙人', '采取', '里面', '重大', '重新', '重要', '鉴于', '问题', '防止', '阿', '附近', '限制', '除', '除了', '除此之外', '除非', '随', '随着', '随著', '集中', '需要', '非但', '非常', '非徒', '靠', '顺', '顺着', '首先', '高兴', '是不是']\n"
          ]
        }
      ],
      "source": [
        "print(stopwords.words('chinese'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88QScAUs0tgy"
      },
      "source": [
        "Let's try with short sentence first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Irr1MAxD0tgy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Words: ['This', 'is', 'an', 'example', 'sentence', 'with', 'some', 'stopwords', '.']\n",
            "\n",
            "Filtered Words: ['example', 'sentence', 'stopwords', '.']\n"
          ]
        }
      ],
      "source": [
        "# Example text\n",
        "text = \"This is an example sentence with some stopwords.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Get the English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Print the original and filtered words\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nFiltered Words:\", filtered_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsz89Gk20tgy"
      },
      "source": [
        "Let's use the tokenized words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2jsHmpxx0tgx"
      },
      "outputs": [],
      "source": [
        "# Sample text\n",
        "text = \"The amber droplet hung from the branch, reaching fullness and ready to drop. \\\n",
        "It waited. While many of the other droplets were satisfied to form as big as they could and release, \\\n",
        "this droplet had other plans. It wanted to be part of history. \\\n",
        "It wanted to be remembered long after all the other droplets had dissolved into history. \\\n",
        "So it waited for the perfect specimen to fly by to trap and \\\n",
        "capture that it hoped would eventually be discovered hundreds of years in the future.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D5jk9RS00tgy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Words: ['The', 'amber', 'droplet', 'hung', 'from', 'the', 'branch', ',', 'reaching', 'fullness', 'and', 'ready', 'to', 'drop', '.', 'It', 'waited', '.', 'While', 'many', 'of', 'the', 'other', 'droplets', 'were', 'satisfied', 'to', 'form', 'as', 'big', 'as', 'they', 'could', 'and', 'release', ',', 'this', 'droplet', 'had', 'other', 'plans', '.', 'It', 'wanted', 'to', 'be', 'part', 'of', 'history', '.', 'It', 'wanted', 'to', 'be', 'remembered', 'long', 'after', 'all', 'the', 'other', 'droplets', 'had', 'dissolved', 'into', 'history', '.', 'So', 'it', 'waited', 'for', 'the', 'perfect', 'specimen', 'to', 'fly', 'by', 'to', 'trap', 'and', 'capture', 'that', 'it', 'hoped', 'would', 'eventually', 'be', 'discovered', 'hundreds', 'of', 'years', 'in', 'the', 'future', '.']\n",
            "\n",
            "Filtered Words: ['amber', 'droplet', 'hung', 'branch', ',', 'reaching', 'fullness', 'ready', 'drop', '.', 'waited', '.', 'many', 'droplets', 'satisfied', 'form', 'big', 'could', 'release', ',', 'droplet', 'plans', '.', 'wanted', 'part', 'history', '.', 'wanted', 'remembered', 'long', 'droplets', 'dissolved', 'history', '.', 'waited', 'perfect', 'specimen', 'fly', 'trap', 'capture', 'hoped', 'would', 'eventually', 'discovered', 'hundreds', 'years', 'future', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the text\n",
        "tokens_nltk = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens_nltk = [word for word in tokens_nltk if word.lower() not in stop_words]\n",
        "\n",
        "# Print the original and filtered words\n",
        "print(\"Original Words:\", tokens_nltk)\n",
        "print(\"\\nFiltered Words:\", filtered_tokens_nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64DdA5Fd0tgy"
      },
      "source": [
        "Compare number of words left after stopwords have been removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wZfCVWqH0tgy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Original Words: 94\n",
            "\n",
            "Number of Filtered Words: 48\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of Original Words:\", len(tokens_nltk))\n",
        "print(\"\\nNumber of Filtered Words:\", len(filtered_tokens_nltk))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GiS_df90tgz"
      },
      "source": [
        "#### HTML tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aL5IZdo-0tgz"
      },
      "outputs": [],
      "source": [
        "# Example HTML text with tags and formatting\n",
        "html_text = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "<title>Sample HTML</title>\n",
        "</head>\n",
        "<body>\n",
        "<h1>Welcome to my website</h1>\n",
        "<p>This is a <b>sample</b> paragraph with <i>formatting</i>.</p>\n",
        "<p>Here's a <a href=\"https://example.com\">link</a> to another page.</p>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o5804YsE0tgz"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def clean_html_tags(text):\n",
        "    # Initialize BeautifulSoup with the HTML text\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "\n",
        "    # Remove all HTML tags\n",
        "    clean_text = soup.get_text(separator=' ')\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HzAmP81j0tgz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample HTML Welcome to my website This is a sample paragraph with formatting . Here's a link to another page.\n"
          ]
        }
      ],
      "source": [
        "cleaned_text = clean_html_tags(html_text)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0DL2-Sf0tgz"
      },
      "source": [
        "Remember this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YBRHMdbetqKB"
      },
      "outputs": [],
      "source": [
        "s = (\"Good muffins cost $3.88\\nin New York.  Please buy me\\n\"\n",
        "      \"two of them.\\n\\nThanks.\")\n",
        "s2 = (\"Alas, it has not rained today. When, do you think, \"\n",
        "      \"will it rain again?\")\n",
        "s3 = (\"<p>Although this is <b>not</b> the case here, we must \"\n",
        "      \"not relax our vigilance!</p>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KJbaT_KxtqKB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Alas, it has not rained today. When, do you think, will it rain again?'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CHY4sfVGtqKB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Alas',\n",
              " 'it has not rained today',\n",
              " 'When',\n",
              " 'do you think',\n",
              " 'will it rain again']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.regexp_tokenize(s2, r'[,\\.\\?!\"]\\s*', gaps=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pl5yqQBGtqKB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<p>Although this is <b>not</b> the case here, we must not relax our vigilance!</p>'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uaqFVGG4tqKB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<p>', '<b>', '</b>', '</p>']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.regexp_tokenize(s3, r'</?.>', gaps=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bpgMNb-OtqKC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Although this is ',\n",
              " 'not',\n",
              " ' the case here, we must not relax our vigilance!']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.regexp_tokenize(s3, r'</?.>', gaps=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajukL_8S0tgz"
      },
      "source": [
        "#### Special characters\n",
        "\n",
        "- Punctuation\n",
        "- '!', '@', '#', '$', '%', '^', '&', '*'\n",
        "- Non-Alphanumeric Characters\n",
        "- Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EMo8JDO20tg0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello How are you doing Im doing fine thank you\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def clean_punctuation(text):\n",
        "    # Define a regular expression pattern to match punctuation and special characters\n",
        "    # Matches any character that is not a word character (\\w), space (\\s), or underscore (_)\n",
        "    punctuation_pattern = re.compile(r'[^\\w\\s]|_')\n",
        "\n",
        "    # Replace punctuation and special characters with an empty string\n",
        "    cleaned_text = re.sub(punctuation_pattern, '', text)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Example text with punctuation and special characters\n",
        "text_with_punctuation = \"Hello! How are you doing? I'm doing fine, thank you!\"\n",
        "\n",
        "# Clean punctuation and special characters\n",
        "cleaned_text = clean_punctuation(text_with_punctuation)\n",
        "print(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obmAJZQB0tg0"
      },
      "source": [
        "### 1.2) Character Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8qVpJz7l0tg0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cafe au lait\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "\n",
        "def normalize_characters(text):\n",
        "    # Normalize text by decomposing Unicode characters and then removing combining characters\n",
        "    normalized_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "\n",
        "    # Convert text to lowercase\n",
        "    normalized_text = normalized_text.lower()\n",
        "\n",
        "    return normalized_text\n",
        "\n",
        "# Example text with accented characters\n",
        "text_with_accented_characters = \"Café au Lait\"\n",
        "\n",
        "# Normalize characters\n",
        "normalized_text = normalize_characters(text_with_accented_characters)\n",
        "print(normalized_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Rax6Wq5h2OLm"
      },
      "outputs": [],
      "source": [
        "# !pip install textacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JxGd43lo0tg0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "café au lait\n"
          ]
        }
      ],
      "source": [
        "from textacy import preprocessing\n",
        "\n",
        "def normalize_characters_with_textacy(text):\n",
        "    # Normalize text using Textacy's normalize_whitespace function\n",
        "    normalized_text = preprocessing.normalize.whitespace(text)\n",
        "\n",
        "    # Lowercase the text\n",
        "    normalized_text = normalized_text.lower()\n",
        "\n",
        "    return normalized_text\n",
        "\n",
        "# Example text with accented characters\n",
        "text_with_accented_characters = \"Café au Lait\"\n",
        "\n",
        "# Normalize characters using Textacy\n",
        "normalized_text = normalize_characters_with_textacy(text_with_accented_characters)\n",
        "print(normalized_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPrdjh0d0tg0"
      },
      "source": [
        "### 1.3) Data Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4E8sebaj0tg0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please contact me at [EMAIL] or [PHONE]. Thank you!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def mask_email_addresses(text):\n",
        "    # Define a regular expression pattern to match email addresses\n",
        "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
        "\n",
        "    # Replace email addresses with a generic placeholder\n",
        "    masked_text = re.sub(email_pattern, '[EMAIL]', text)\n",
        "\n",
        "    return masked_text\n",
        "\n",
        "def mask_phone_numbers(text):\n",
        "    # Define a regular expression pattern to match phone numbers\n",
        "    phone_pattern = re.compile(r'(\\d{3})-(\\d{3})-(\\d{4})')\n",
        "\n",
        "    # Replace phone numbers with a generic placeholder\n",
        "    masked_text = re.sub(phone_pattern, '[PHONE]', text)\n",
        "\n",
        "    return masked_text\n",
        "\n",
        "# Example text with email addresses and phone numbers\n",
        "text_with_sensitive_info = \"Please contact me at john.doe@example.com or 123-456-7890. Thank you!\"\n",
        "\n",
        "# Mask sensitive information\n",
        "masked_text = mask_email_addresses(text_with_sensitive_info)\n",
        "masked_text = mask_phone_numbers(masked_text)\n",
        "print(masked_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ODGES110tg0"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkYTtUVY0tg1"
      },
      "source": [
        "## 2) Text Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qyef4WH0tg1"
      },
      "source": [
        "### 2.1) Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QC62vc9_0tg1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The amber droplet hung from the branch, reaching fullness and ready to drop. It waited. While many of the other droplets were satisfied to form as big as they could and release, this droplet had other plans. It wanted to be part of history. It wanted to be remembered long after all the other droplets had dissolved into history. So it waited for the perfect specimen to fly by to trap and capture that it hoped would eventually be discovered hundreds of years in the future.\n"
          ]
        }
      ],
      "source": [
        "text = \"The amber droplet hung from the branch, reaching fullness and ready to drop. \\\n",
        "It waited. While many of the other droplets were satisfied to form as big as they could and release, \\\n",
        "this droplet had other plans. It wanted to be part of history. \\\n",
        "It wanted to be remembered long after all the other droplets had dissolved into history. \\\n",
        "So it waited for the perfect specimen to fly by to trap and \\\n",
        "capture that it hoped would eventually be discovered hundreds of years in the future.\"\n",
        "\n",
        "# Before tokenization\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UMoudGK0tg1"
      },
      "source": [
        "Implementation from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2j842BOD0tg1"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "# Convert text to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Split the text into words\n",
        "tokens = text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SN1O_XHY0tg2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LrHymTvm0tg2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the',\n",
              " 'amber',\n",
              " 'droplet',\n",
              " 'hung',\n",
              " 'from',\n",
              " 'the',\n",
              " 'branch',\n",
              " 'reaching',\n",
              " 'fullness',\n",
              " 'and',\n",
              " 'ready',\n",
              " 'to',\n",
              " 'drop',\n",
              " 'it',\n",
              " 'waited',\n",
              " 'while',\n",
              " 'many',\n",
              " 'of',\n",
              " 'the',\n",
              " 'other',\n",
              " 'droplets',\n",
              " 'were',\n",
              " 'satisfied',\n",
              " 'to',\n",
              " 'form',\n",
              " 'as',\n",
              " 'big',\n",
              " 'as',\n",
              " 'they',\n",
              " 'could',\n",
              " 'and',\n",
              " 'release',\n",
              " 'this',\n",
              " 'droplet',\n",
              " 'had',\n",
              " 'other',\n",
              " 'plans',\n",
              " 'it',\n",
              " 'wanted',\n",
              " 'to',\n",
              " 'be',\n",
              " 'part',\n",
              " 'of',\n",
              " 'history',\n",
              " 'it',\n",
              " 'wanted',\n",
              " 'to',\n",
              " 'be',\n",
              " 'remembered',\n",
              " 'long',\n",
              " 'after',\n",
              " 'all',\n",
              " 'the',\n",
              " 'other',\n",
              " 'droplets',\n",
              " 'had',\n",
              " 'dissolved',\n",
              " 'into',\n",
              " 'history',\n",
              " 'so',\n",
              " 'it',\n",
              " 'waited',\n",
              " 'for',\n",
              " 'the',\n",
              " 'perfect',\n",
              " 'specimen',\n",
              " 'to',\n",
              " 'fly',\n",
              " 'by',\n",
              " 'to',\n",
              " 'trap',\n",
              " 'and',\n",
              " 'capture',\n",
              " 'that',\n",
              " 'it',\n",
              " 'hoped',\n",
              " 'would',\n",
              " 'eventually',\n",
              " 'be',\n",
              " 'discovered',\n",
              " 'hundreds',\n",
              " 'of',\n",
              " 'years',\n",
              " 'in',\n",
              " 'the',\n",
              " 'future']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# After tokenization\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7luZBwat0tg2"
      },
      "source": [
        "Using NLTK library\n",
        "\n",
        "```from nltk.tokenize import word_tokenize```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ChF4Sd3_0tg2"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the text\n",
        "tokens_nltk = word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "p_gZpHv10tg2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokens_nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYTpbAGB0tg2"
      },
      "source": [
        "### 2.2) Lemmatization/Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnSqZJXg0tg2"
      },
      "source": [
        "Use NLTK library ```WordNetLemmatizer```\n",
        "\n",
        "```\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tWfp0ICl0tg2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFPpcCqk0tg2"
      },
      "source": [
        "Let's first try stemming some words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Dgxz5Gtb0tg2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n",
            "running : run\n",
            "swam : swim\n",
            "swum : swim\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
        "\n",
        "# a denotes verb in \"pos\"\n",
        "print(\"running :\", lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
        "\n",
        "# a denotes verb in \"pos\"\n",
        "print(\"swam :\", lemmatizer.lemmatize(\"swam\", pos=\"v\"))\n",
        "\n",
        "print(\"swum :\", lemmatizer.lemmatize(\"swum\", pos=\"v\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6CGbekT0tg2"
      },
      "source": [
        "Use Spacy library\n",
        "\n",
        "*Note: Use the following command if getting a linkage error.*\n",
        "\n",
        "```python -m spacy download en```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "11L-AYxH0tg3"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "dTQ5VTyS0tg3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text: Three men are walking.\n",
            "\n",
            "Lemmatized Text: three man be walk .\n"
          ]
        }
      ],
      "source": [
        "# Define a sample text\n",
        "text = \"Three men are walking.\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract lemmatized tokens\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "# Join the lemmatized tokens into a sentence\n",
        "lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Print the original and lemmatized text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"\\nLemmatized Text:\", lemmatized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oypKhlNd0tg3"
      },
      "source": [
        "Let's use longer sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "KPLt-SdM0tg3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text: The amber droplet hung from the branch, reaching fullness and ready to drop. It waited. While many of the other droplets were satisfied to form as big as they could and release, this droplet had other plans. It wanted to be part of history. It wanted to be remembered long after all the other droplets had dissolved into history. So it waited for the perfect specimen to fly by to trap and capture that it hoped would eventually be discovered hundreds of years in the future.\n",
            "\n",
            "Lemmatized Text: the amber droplet hang from the branch , reach fullness and ready to drop . it wait . while many of the other droplet be satisfied to form as big as they could and release , this droplet have other plan . it want to be part of history . it want to be remember long after all the other droplet have dissolve into history . so it wait for the perfect speciman to fly by to trap and capture that it hope would eventually be discover hundred of year in the future .\n"
          ]
        }
      ],
      "source": [
        "text = \"The amber droplet hung from the branch, reaching fullness and ready to drop. \\\n",
        "It waited. While many of the other droplets were satisfied to form as big as they could and release, \\\n",
        "this droplet had other plans. It wanted to be part of history. \\\n",
        "It wanted to be remembered long after all the other droplets had dissolved into history. \\\n",
        "So it waited for the perfect specimen to fly by to trap and \\\n",
        "capture that it hoped would eventually be discovered hundreds of years in the future.\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract lemmatized tokens\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "# Join the lemmatized tokens into a sentence\n",
        "lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Print the original and lemmatized text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"\\nLemmatized Text:\", lemmatized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-peGuyA0tg3"
      },
      "source": [
        "### 2.3) Part-of-speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fF3IzsgW0tg3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('I', 'PRON'), ('love', 'VERB'), ('eating', 'VERB'), ('pizza', 'NOUN'), ('with', 'ADP'), ('my', 'PRON'), ('friends', 'NOUN')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pos_tagging(text):\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract POS tags for each token in the text\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "    return pos_tags\n",
        "\n",
        "# Example text\n",
        "text = \"I love eating pizza with my friends\"\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = pos_tagging(text)\n",
        "print(pos_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br1YEKPl0tg3"
      },
      "source": [
        "### 2.4) Name-Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "eI09UMtv0tg3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('KMUTT', 'ORG'), ('Bangkok', 'GPE'), ('Thailand', 'GPE')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def ner(text):\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract named entities from the processed text\n",
        "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "\n",
        "    return entities\n",
        "\n",
        "# Example text\n",
        "text = \"KMUTT is located in Bangkok, Thailand.\"\n",
        "\n",
        "# Perform NER\n",
        "named_entities = ner(text)\n",
        "print(named_entities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ORG: Organization\\\n",
        "GPE: Geopolitical Entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
